diff --git a/mm/slob.c b/mm/slob.c
index 837ebd6..948c3fc 100644
--- a/mm/slob.c
+++ b/mm/slob.c
@@ -1,4 +1,16 @@
 /*
+ * Project 3
+ *   A best-fit memory allocator
+ *
+ * Team 3
+ * Authors:
+ *   Alex Weeks
+ *   Josh Jordahl
+ *   Kevin McIntosh
+ *   Tyler McClung
+ */
+
+/*
  * SLOB Allocator: Simple List Of Blocks
  *
  * Matt Mackall <mpm@selenic.com> 12/30/03
@@ -69,6 +81,7 @@
 #include <linux/kmemtrace.h>
 #include <linux/kmemleak.h>
 #include <asm/atomic.h>
+#include <linux/syscalls.h>
 
 /*
  * slob_block has a field 'units', which indicates size of block if +ve,
@@ -121,12 +134,18 @@ static inline void free_slob_page(struct slob_page *sp)
 
 /*
  * All partially free slob pages go on these lists.
- */
+ 
 #define SLOB_BREAK1 256
 #define SLOB_BREAK2 1024
 static LIST_HEAD(free_slob_small);
-static LIST_HEAD(free_slob_medium);
-static LIST_HEAD(free_slob_large);
+static LIST_HEAD(free_slob_medium);*/
+static LIST_HEAD(slob_list);
+
+
+/*
+ *  For tracking memory utilization
+ */
+size_t slob_amt_claimed = 0, slob_amt_free = 0;
 
 /*
  * is_slob_page: True for all slob pages (false for bigblock pages)
@@ -262,55 +281,104 @@ static void slob_free_pages(void *b, int order)
 	free_pages((unsigned long)b, order);
 }
 
+/* 
+ * Get the best fit size internal to a page
+ */
+size_t get_best_fit_size(struct slob_page *sp, size_t size, int align){
+	int best_fit = 0;
+	
+	slob_t *prev, *cur, *aligned = NULL;
+	int delta = 0, units = SLOB_UNITS(size);
+	
+	for (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) 
+	{
+		slobidx_t avail = slob_units(cur);
+		
+		if (align) 
+		{
+			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
+			delta = aligned - cur;
+		}
+		
+		if (avail >= units + delta && (!best_fit || best_fit > avail))  /* room enough? */
+		{
+			best_fit = avail;
+		}
+        if (slob_last(cur))
+            return 0;
+	}
+	
+	return best_fit;
+}
+
 /*
  * Allocate a slob block within a given slob_page sp.
  */
 static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
 {
-	slob_t *prev, *cur, *aligned = NULL;
+	int best_fit = 0;
+	
+	slob_t *prev, *next, *cur, *aligned, *best_block = NULL;
+	
 	int delta = 0, units = SLOB_UNITS(size);
-
-	for (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) {
-		slobidx_t avail = slob_units(cur);
-
-		if (align) {
+	slobidx_t avail;
+	
+	for (prev = NULL, cur = sp->free; ; prev = cur, cur = slob_next(cur)) 
+	{
+		avail = slob_units(cur);
+		
+		if (align) 
+		{
 			aligned = (slob_t *)ALIGN((unsigned long)cur, align);
 			delta = aligned - cur;
 		}
-		if (avail >= units + delta) { /* room enough? */
-			slob_t *next;
-
-			if (delta) { /* need to fragment head to align? */
-				next = slob_next(cur);
-				set_slob(aligned, avail - delta, next);
-				set_slob(cur, delta, aligned);
-				prev = cur;
-				cur = aligned;
-				avail = slob_units(cur);
-			}
-
-			next = slob_next(cur);
-			if (avail == units) { /* exact fit? unlink. */
-				if (prev)
-					set_slob(prev, slob_units(prev), next);
-				else
-					sp->free = next;
-			} else { /* fragment */
-				if (prev)
-					set_slob(prev, slob_units(prev), cur + units);
-				else
-					sp->free = cur + units;
-				set_slob(cur + units, avail - units, next);
-			}
-
-			sp->units -= units;
-			if (!sp->units)
-				clear_slob_page_free(sp);
-			return cur;
+		
+		if (avail >= units + delta && (!best_fit || best_fit > avail))  /* room enough? */
+		{
+			best_block = cur;
+			best_fit = avail;
+
 		}
-		if (slob_last(cur))
-			return NULL;
+        	if (slob_last(cur))
+           		break;
+	}
+	
+	avail = best_fit;
+	cur = best_block;
+	if(!best_fit)
+		return NULL;
+    
+    /* Record memory as claimed */    
+    slob_amt_claimed += SLOB_UNITS(size);
+    slob_amt_free -= SLOB_UNITS(size);
+    	
+	if (delta) { /* need to fragment head to align? */
+		next = slob_next(cur);
+		set_slob(aligned, avail - delta, next);
+		set_slob(cur, delta, aligned);
+		prev = cur;
+		cur = aligned;
+		avail = slob_units(cur);
+	}
+	next = slob_next(cur);
+	if (avail == units) { /* exact fit? unlink. */
+		if (prev)
+			set_slob(prev, slob_units(prev), next);
+		else
+			sp->free = next;
+	} else { /* fragment */
+		if (prev)
+			set_slob(prev, slob_units(prev), cur + units);
+		else
+			sp->free = cur + units;
+		set_slob(cur + units, avail - units, next);
 	}
+
+	sp->units -= units;
+	if (!sp->units)
+		clear_slob_page_free(sp);
+	
+	return cur;
 }
 
 /*
@@ -319,67 +387,67 @@ static void *slob_page_alloc(struct slob_page *sp, size_t size, int align)
 static void *slob_alloc(size_t size, gfp_t gfp, int align, int node)
 {
 	struct slob_page *sp;
-	struct list_head *prev;
-	struct list_head *slob_list;
+
 	slob_t *b = NULL;
 	unsigned long flags;
-
-	if (size < SLOB_BREAK1)
-		slob_list = &free_slob_small;
-	else if (size < SLOB_BREAK2)
-		slob_list = &free_slob_medium;
-	else
-		slob_list = &free_slob_large;
+    
+    struct slob_page *best_fit_page;
+	size_t curr_size, best_size = 0;
 
 	spin_lock_irqsave(&slob_lock, flags);
-	/* Iterate through each partially free page, try to find room */
-	list_for_each_entry(sp, slob_list, list) {
+		/* Iterate through each partially free page, try to find room */
+	list_for_each_entry(sp, &slob_list, list) {
 #ifdef CONFIG_NUMA
 		/*
 		 * If there's a node specification, search for a partial
 		 * page with a matching node id in the freelist.
 		 */
+		 
 		if (node != -1 && page_to_nid(&sp->page) != node)
 			continue;
 #endif
+		
 		/* Enough room on this page? */
 		if (sp->units < SLOB_UNITS(size))
 			continue;
 
-		/* Attempt to alloc */
-		prev = sp->list.prev;
-		b = slob_page_alloc(sp, size, align);
-		if (!b)
-			continue;
-
-		/* Improve fragment distribution and reduce our average
-		 * search time by starting our next search here. (see
-		 * Knuth vol 1, sec 2.5, pg 449) */
-		if (prev != slob_list->prev &&
-				slob_list->next != prev->next)
-			list_move_tail(slob_list, prev->next);
-		break;
+		/*Get the best size*/
+		curr_size = get_best_fit_size(sp, size, align);
+        	if( curr_size < best_size ) {
+            		best_size      = curr_size;
+            		best_fit_page  = sp;
+        	}
 	}
+	
+	if (best_size)
+		slob_page_alloc(best_fit_page, best_size, align);
+		
 	spin_unlock_irqrestore(&slob_lock, flags);
 
 	/* Not enough space: must allocate a new page */
-	if (!b) {
+	if (!best_size) {
 		b = slob_new_pages(gfp & ~__GFP_ZERO, 0, node);
+		
 		if (!b)
 			return NULL;
 		sp = slob_page(b);
 		set_slob_page(sp);
-
+		
 		spin_lock_irqsave(&slob_lock, flags);
 		sp->units = SLOB_UNITS(PAGE_SIZE);
-		sp->free = b;
+        
+        slob_amt_free += sp->units;
+		
+        sp->free = b;
 		INIT_LIST_HEAD(&sp->list);
 		set_slob(b, SLOB_UNITS(PAGE_SIZE), b + SLOB_UNITS(PAGE_SIZE));
-		set_slob_page_free(sp, slob_list);
+		set_slob_page_free(sp, &slob_list);
 		b = slob_page_alloc(sp, size, align);
+		if (!b)
 		BUG_ON(!b);
 		spin_unlock_irqrestore(&slob_lock, flags);
 	}
+	
 	if (unlikely((gfp & __GFP_ZERO) && b))
 		memset(b, 0, size);
 	return b;
@@ -403,6 +471,8 @@ static void slob_free(void *block, int size)
 	units = SLOB_UNITS(size);
 
 	spin_lock_irqsave(&slob_lock, flags);
+    
+    slob_amt_claimed -= units;
 
 	if (sp->units + units == SLOB_UNITS(PAGE_SIZE)) {
 		/* Go directly to page allocator. Do not pass slob allocator */
@@ -412,7 +482,9 @@ static void slob_free(void *block, int size)
 		clear_slob_page(sp);
 		free_slob_page(sp);
 		slob_free_pages(b, 0);
-		return;
+		
+        slob_amt_free -= SLOB_UNITS(PAGE_SIZE);
+        return;
 	}
 
 	if (!slob_page_free(sp)) {
@@ -422,8 +494,8 @@ static void slob_free(void *block, int size)
 		set_slob(b, units,
 			(void *)((unsigned long)(b +
 					SLOB_UNITS(PAGE_SIZE)) & PAGE_MASK));
-		set_slob_page_free(sp, &free_slob_small);
-		goto out;
+		set_slob_page_free(sp, &slob_list);
+        goto out;
 	}
 
 	/*
@@ -697,3 +769,13 @@ void __init kmem_cache_init_late(void)
 {
 	/* Nothing to do */
 }
+
+SYSCALL_DEFINE0(get_slob_amt_claimed)
+{
+    return slob_amt_claimed;
+}
+
+SYSCALL_DEFINE0(get_slob_amt_free)
+{
+    return slob_amt_free;
+}
